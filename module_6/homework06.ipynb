{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cb0645",
   "metadata": {},
   "source": [
    "# Homework06 - Rui Pinto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0936962",
   "metadata": {},
   "source": [
    "## Q1. Refactoring\n",
    "\n",
    "Before we can start covering our code with tests, we need to \n",
    "refactor it. We'll start by getting rid of all the global variables. \n",
    "\n",
    "* Let's create a function `main` with two parameters: `year` and\n",
    "`month`.\n",
    "* Move all the code (except `read_data`) inside `main`\n",
    "* Make `categorical` a parameter for `read_data` and pass it inside `main`\n",
    "\n",
    "Now we need to create the \"main\" block from which we'll invoke\n",
    "the main function. How does the `if` statement that we use for\n",
    "this looks like? \n",
    "\n",
    "\n",
    "Hint: after refactoring, check that the code still works. Just run it e.g. for March 2023 and see if it finishes successfully. \n",
    "\n",
    "To make it easier to run it, you can write results to your local\n",
    "filesystem. E.g. here:\n",
    "\n",
    "```python\n",
    "output_file = f'taxi_type=yellow_year={year:04d}_month={month:02d}.parquet'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "from homework06.batch_refactoring import main\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# Use a local path for output to test Q1 (no S3 needed)\n",
    "# Temporarily unset any environment variables that might affect the path\n",
    "if \"INPUT_FILE_PATTERN\" in os.environ:\n",
    "    del os.environ[\"INPUT_FILE_PATTERN\"]\n",
    "if \"OUTPUT_FILE_PATTERN\" in os.environ:\n",
    "    del os.environ[\"OUTPUT_FILE_PATTERN\"]\n",
    "if \"S3_ENDPOINT_URL\" in os.environ:\n",
    "    del os.environ[\"S3_ENDPOINT_URL\"]\n",
    "\n",
    "# Now run the main function which will use the default paths\n",
    "output_path = main(2023, 3)\n",
    "print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c667e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ANSWER TO Q1: The correct if statement for the 'main' block is:\")\n",
    "print('if __name__ == \"__main__\":')\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1bf73",
   "metadata": {},
   "source": [
    "## Q2. Installing pytest\n",
    "\n",
    "Now we need to install `pytest`:\n",
    "\n",
    "```bash\n",
    "uv add --dev pytest\n",
    "```\n",
    "\n",
    "Next, create a folder `tests` and create two files. One will be\n",
    "the file with tests. We can name it `test_batch_refactoring.py`. \n",
    "\n",
    "What should be the other file? \n",
    "\n",
    "Hint: to be able to test `batch_refactoring.py`, we need to be able to\n",
    "import it. Without this other file, we won't be able to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab572d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if not already installed\n",
    "# !uv add --dev pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"homework06/tests\", exist_ok=True)\n",
    "\n",
    "# create the files inside the tests folder\n",
    "!touch homework06/tests/test_batch_refactoring.py\n",
    "!touch homework06/tests/__init__.py\n",
    "\n",
    "!ls -l homework06/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ANSWER TO Q2: The other file needed in the tests directory is __init__.py\")\n",
    "print(\n",
    "    \"This file allows Python to recognize the tests directory as a package, enabling imports.\"\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c431dd6",
   "metadata": {},
   "source": [
    "## Q3. Writing first unit test\n",
    "\n",
    "Now let's cover our code with unit tests.\n",
    "\n",
    "We'll start with the pre-processing logic inside `read_data`.\n",
    "\n",
    "It's difficult to test right now because first reads\n",
    "the file and then performs some transformations. We need to split this \n",
    "code into two parts: reading (I/O) and transformation. \n",
    "\n",
    "So let's create a function `prepare_data` that takes in a dataframe \n",
    "(and some other parameters too) and applies some transformation to it.\n",
    "\n",
    "(That's basically the entire `read_data` function after reading \n",
    "the parquet file)\n",
    "\n",
    "Now create a test and use this as input:\n",
    "\n",
    "```python\n",
    "data = [\n",
    "    (None, None, dt(1, 1), dt(1, 10)),\n",
    "    (1, 1, dt(1, 2), dt(1, 10)),\n",
    "    (1, None, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "    (3, 4, dt(1, 2, 0), dt(2, 2, 1)),      \n",
    "]\n",
    "\n",
    "columns = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "```\n",
    "\n",
    "Where `dt` is a helper function:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2023, 1, 1, hour, minute, second)\n",
    "```\n",
    "\n",
    "Define the expected output and use the assert to make sure \n",
    "that the actual dataframe matches the expected one.\n",
    "\n",
    "Tip: When you compare two Pandas DataFrames, the result is also a DataFrame.\n",
    "The same is true for Pandas Series. Also, a DataFrame could be turned into a list of dictionaries.  \n",
    "\n",
    "How many rows should be there in the expected dataframe?\n",
    "\n",
    "* 1\n",
    "* 2 ✅\n",
    "* 3\n",
    "* 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62565eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pytest from the notebook\n",
    "import pytest\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def run_test():\n",
    "    # Capture stdout to get test results\n",
    "    output = StringIO()\n",
    "    sys.stdout = output\n",
    "\n",
    "    # Run the test\n",
    "    pytest.main([\"-xvs\", \"homework06/tests/test_batch_refactoring.py\"])\n",
    "\n",
    "    # Restore stdout and return results\n",
    "    sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RUNNING UNIT TEST FOR Q3\")\n",
    "print(\"=\" * 50)\n",
    "test_output = run_test()\n",
    "print(test_output)\n",
    "\n",
    "# Extract and display the answer explicitly\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANSWER TO Q3: The expected dataframe should have 2 rows ✅\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e19162",
   "metadata": {},
   "source": [
    "## Q4. Mocking S3 with Localstack \n",
    "\n",
    "Now let's prepare for an integration test. In our script, we \n",
    "write data to S3. So we'll use Localstack to mimic S3.\n",
    "\n",
    "First, let's run Localstack with Docker compose. Let's create a \n",
    "`docker-compose.yaml` file with just one service: localstack. Inside\n",
    "localstack, we're only interested in running S3. \n",
    "\n",
    "Start the service and test it by creating a bucket where we'll\n",
    "keep the output. Let's call it \"nyc-duration\".\n",
    "\n",
    "With AWS CLI, this is how we create a bucket:\n",
    "\n",
    "```bash\n",
    "aws s3 mb s3://nyc-duration\n",
    "```\n",
    "\n",
    "Then we need to check that the bucket was successfully created. With AWS, this is how we typically do it:\n",
    "\n",
    "```bash\n",
    "aws s3 ls\n",
    "```\n",
    "\n",
    "In both cases we should adjust commands for localstack. What option do we need to use for such purposes?\n",
    "\n",
    "* `--backend-store-uri`\n",
    "* `--profile`\n",
    "* `--endpoint-url` ✅\n",
    "* `--version`\n",
    "\n",
    "\n",
    "## Make input and output paths configurable\n",
    "\n",
    "Right now the input and output paths are hardcoded, but we want\n",
    "to change it for the tests. \n",
    "\n",
    "One of the possible ways would be to specify `INPUT_FILE_PATTERN` and `OUTPUT_FILE_PATTERN` via the env \n",
    "variables. Let's do that:\n",
    "\n",
    "\n",
    "```bash\n",
    "export INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "export OUTPUT_FILE_PATTERN=\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "```\n",
    "\n",
    "And this is how we can read them:\n",
    "\n",
    "```python\n",
    "def get_input_path(year, month):\n",
    "    default_input_pattern = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    input_pattern = os.getenv('INPUT_FILE_PATTERN', default_input_pattern)\n",
    "    return input_pattern.format(year=year, month=month)\n",
    "\n",
    "\n",
    "def get_output_path(year, month):\n",
    "    default_output_pattern = 's3://nyc-duration-prediction-alexey/taxi_type=fhv/year={year:04d}/month={month:02d}/predictions.parquet'\n",
    "    output_pattern = os.getenv('OUTPUT_FILE_PATTERN', default_output_pattern)\n",
    "    return output_pattern.format(year=year, month=month)\n",
    "\n",
    "\n",
    "def main(year, month):\n",
    "    input_file = get_input_path(year, month)\n",
    "    output_file = get_output_path(year, month)\n",
    "    # rest of the main function ... \n",
    "```\n",
    "\n",
    "\n",
    "## Reading from Localstack S3 with Pandas\n",
    "\n",
    "So far we've been reading parquet files from S3 with using\n",
    "pandas `read_parquet`. But this way we read it from the\n",
    "actual S3 service. Now we need to replace it with our localstack\n",
    "one.\n",
    "\n",
    "For that, we need to specify the endpoint url:\n",
    "\n",
    "```python\n",
    "options = {\n",
    "    'client_kwargs': {\n",
    "        'endpoint_url': S3_ENDPOINT_URL\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.read_parquet('s3://bucket/file.parquet', storage_options=options)\n",
    "```\n",
    "\n",
    "Let's modify our `read_data` function:\n",
    "\n",
    "- check if `S3_ENDPOINT_URL` is set, and if it is, use it for reading\n",
    "- otherwise use the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure paths for local testing with LocalStack\n",
    "os.environ[\"INPUT_FILE_PATTERN\"] = \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "os.environ[\"OUTPUT_FILE_PATTERN\"] = (\n",
    "    \"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    ")\n",
    "os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "\n",
    "print(f\"S3_ENDPOINT_URL: {os.environ['S3_ENDPOINT_URL']}\")\n",
    "print(f\"OUTPUT_FILE_PATTERN: {os.environ['OUTPUT_FILE_PATTERN']}\")\n",
    "print(f\"INPUT_FILE_PATTERN: {os.environ['INPUT_FILE_PATTERN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the S3 bucket exists before running tests\n",
    "!python homework06/tests/setup_s3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the LocalStack bucket exists\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING S3 FOR Q4\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!aws --endpoint-url=http://localhost:4566 s3 ls\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\n",
    "    \"ANSWER TO Q4: The correct option to use with AWS CLI for LocalStack is '--endpoint-url' ✅\"\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c05dcf",
   "metadata": {},
   "source": [
    "## Q5. Creating test data\n",
    "\n",
    "Now let's create `integration_test.py`\n",
    "\n",
    "We'll use the dataframe we created in Q3 (the dataframe for the unit test)\n",
    "and save it to S3. You don't need to do anything else: just create a dataframe \n",
    "and save it.\n",
    "\n",
    "We will pretend that this is data for January 2023.\n",
    "\n",
    "Run the `integration_test.py` script. After that, use AWS CLI to verify that the \n",
    "file was created. \n",
    "\n",
    "Use this snipped for saving the file:\n",
    "\n",
    "```python\n",
    "df_input.to_parquet(\n",
    "    input_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False,\n",
    "    storage_options=options\n",
    ")\n",
    "```\n",
    "\n",
    "What's the size of the file?\n",
    "\n",
    "* 3620 ✅\n",
    "* 23620\n",
    "* 43620\n",
    "* 63620\n",
    "\n",
    "Note: it's important to use the code from the snippet for saving\n",
    "the file. Otherwise the size may be different depending on the OS,\n",
    "engine and compression. Even if you use this exact snippet, the size\n",
    "of your dataframe may still be a bit off. Just select the closest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the file size of the test data file\n",
    "import os\n",
    "\n",
    "# Configure the S3 endpoint and paths\n",
    "os.environ[\"INPUT_FILE_PATTERN\"] = \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "# Define the path to the test data file\n",
    "year, month = 2023, 1\n",
    "input_file = os.environ[\"INPUT_FILE_PATTERN\"].format(year=year, month=month)\n",
    "\n",
    "# Use aws cli to get the file size\n",
    "import subprocess\n",
    "\n",
    "cmd = f\"aws --endpoint-url={os.environ['S3_ENDPOINT_URL']} s3 ls {input_file}\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    # Parse the output to get file size\n",
    "    output = result.stdout.strip()\n",
    "    if output:\n",
    "        parts = output.split()\n",
    "        if len(parts) >= 3:\n",
    "            size = parts[2]\n",
    "            print(f\"File size of test data: {size} bytes\")\n",
    "            print(\"\\nAnswer for Q5: The file size is closest to 3620 bytes ✅\")\n",
    "        else:\n",
    "            print(\"Could not parse file size from output:\", output)\n",
    "    else:\n",
    "        print(\"No output returned from aws command\")\n",
    "else:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    print(\n",
    "        \"Make sure to run the integration_test.py script first to create the test data.\"\n",
    "    )\n",
    "    print(\"You can run it with: python homework06/tests/integration_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa76e",
   "metadata": {},
   "source": [
    "## Q6. Finish the integration test\n",
    "\n",
    "We can read from our localstack s3, but we also need to write to it.\n",
    "\n",
    "Let's run the `batch_refactoring.py` script for January 2023 (the fake data\n",
    "we created in Q5).\n",
    "\n",
    "We can do that from our integration test in Python: we can use\n",
    "`os.system` for doing that (there are other options too).\n",
    "\n",
    "Now it saves the result to localstack.\n",
    "\n",
    "The only thing we need to do now is to read this data and \n",
    "verify the result is correct. \n",
    "\n",
    "What's the sum of predicted durations for the test dataframe?\n",
    "\n",
    "* 13.08\n",
    "* 36.28 ✅\n",
    "* 69.28\n",
    "* 81.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13373cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def verify_s3_bucket_exists(bucket_name=\"nyc-duration\"):\n",
    "    \"\"\"Make sure the S3 bucket exists before running the test\"\"\"\n",
    "    s3_endpoint_url = os.getenv(\"S3_ENDPOINT_URL\", \"http://localhost:4566\")\n",
    "\n",
    "    # Create the bucket if it doesn't exist\n",
    "    cmd_check = f\"aws --endpoint-url={s3_endpoint_url} s3 ls s3://{bucket_name} || aws --endpoint-url={s3_endpoint_url} s3 mb s3://{bucket_name}\"\n",
    "    print(\"Verifying S3 bucket exists...\")\n",
    "    subprocess.run(cmd_check, shell=True, check=True)\n",
    "\n",
    "    # Create directories if needed\n",
    "    cmd_dirs = f\"aws --endpoint-url={s3_endpoint_url} s3api put-object --bucket {bucket_name} --key in/\"\n",
    "    subprocess.run(cmd_dirs, shell=True, check=True)\n",
    "    cmd_dirs = f\"aws --endpoint-url={s3_endpoint_url} s3api put-object --bucket {bucket_name} --key out/\"\n",
    "    subprocess.run(cmd_dirs, shell=True, check=True)\n",
    "\n",
    "    # Verify test data file exists\n",
    "    year, month = 2023, 1\n",
    "    input_pattern = os.getenv(\n",
    "        \"INPUT_FILE_PATTERN\", \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    input_file = input_pattern.format(year=year, month=month)\n",
    "\n",
    "    cmd_check_file = f\"aws --endpoint-url={s3_endpoint_url} s3 ls {input_file}\"\n",
    "    result = subprocess.run(cmd_check_file, shell=True, capture_output=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Test data file does not exist: {input_file}\")\n",
    "        print(\"Run the integration_test.py script first to create the test data.\")\n",
    "        print(\"Attempting to run it now...\")\n",
    "        subprocess.run([\"python\", \"homework06/tests/integration_test.py\"], check=True)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_integration():\n",
    "    \"\"\"Integration test for batch prediction\n",
    "\n",
    "    1. First make sure the test data exists in S3\n",
    "    2. Run batch_refactoring.py for January 2023\n",
    "    3. Read the results and calculate sum of predictions\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"RUNNING INTEGRATION TEST FOR Q6\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Configure environment variables\n",
    "    os.environ[\"INPUT_FILE_PATTERN\"] = (\n",
    "        \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    os.environ[\"OUTPUT_FILE_PATTERN\"] = (\n",
    "        \"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "    # Make sure the bucket and test data exist\n",
    "    verify_s3_bucket_exists()\n",
    "\n",
    "    # Set parameters for January 2023\n",
    "    year, month = 2023, 1\n",
    "    s3_endpoint_url = os.environ[\"S3_ENDPOINT_URL\"]\n",
    "\n",
    "    # S3 options for reading/writing with LocalStack\n",
    "    options = {\"client_kwargs\": {\"endpoint_url\": s3_endpoint_url}}\n",
    "\n",
    "    # Import and run the batch prediction script\n",
    "    from homework06.batch_refactoring import main\n",
    "\n",
    "    output_path = main(year, month)\n",
    "    print(f\"Batch prediction completed. Output saved to: {output_path}\")\n",
    "\n",
    "    # Define the output path (should be the same as returned by main)\n",
    "    output_pattern = os.environ[\"OUTPUT_FILE_PATTERN\"]\n",
    "    output_file = output_pattern.format(year=year, month=month)\n",
    "\n",
    "    # Read the results\n",
    "    try:\n",
    "        df_result = pd.read_parquet(output_file, storage_options=options)\n",
    "        print(f\"Successfully read output file: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading output file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Show the results\n",
    "    print(\"\\nOutput DataFrame:\")\n",
    "    print(df_result)\n",
    "\n",
    "    # Calculate sum of predicted durations\n",
    "    sum_pred = df_result[\"predicted_duration\"].sum()\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Sum of predicted durations: {sum_pred:.2f}\")\n",
    "\n",
    "    # Check which option is closest to our result\n",
    "    options = [13.08, 36.28, 69.28, 81.08]\n",
    "    closest = min(options, key=lambda x: abs(x - sum_pred))\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"ANSWER TO Q6: The sum of predicted durations is {sum_pred:.2f}\")\n",
    "    print(f\"The closest option is: {closest} ✅\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Run the integration test\n",
    "test_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68089e8",
   "metadata": {},
   "source": [
    "## Running the test (ungraded)\n",
    "The rest is ready, but we need to write a shell script for doing that.\n",
    "\n",
    "Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the tests directory and run the tests\n",
    "!./run_tests.sh\n",
    "!./run_integration_tests.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-zoomcamp-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
